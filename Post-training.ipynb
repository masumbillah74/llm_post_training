{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dedd51e-d562-4846-9da6-6877dbdcff8f",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d075718-1250-4169-b223-b7db7559e12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "import json\n",
    "import transformers\n",
    "import gradio as gr\n",
    "import spaces\n",
    "from threading import Thread\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46201ea0-d175-43d2-a66b-9e98c737f1e3",
   "metadata": {},
   "source": [
    "## Download and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a390a63c-f3eb-4231-82c7-efa7db0e50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "with open('access_token.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "access_token = data[\"access_token\"]\n",
    "\n",
    "model = transformers.MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"mps\",\n",
    "    token=access_token\n",
    ")\n",
    "\n",
    "processor = transformers.AutoProcessor.from_pretrained(model_id, token=access_token)\n",
    "# Save model locally\n",
    "model.save_pretrained(\"llama\")\n",
    "processor.save_pretrained(\"processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100b980-9a3f-4d2a-8093-b68952c6f06a",
   "metadata": {},
   "source": [
    "## Fine-tune the model with custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6930e8b7-8cb7-48ef-87c2-41ff67fdd212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# 1Ô∏è‚É£ Load LLaMA-3.2-11B-Vision-Instruct Model and Tokenizer\n",
    "# ==============================\n",
    "model_name = \"llama/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('processor/')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding token is set\n",
    "\n",
    "# Load model in MPS (Apple GPU)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,  # Use `bfloat16` if preferred: torch.bfloat16\n",
    "    device_map={\"\": device}  # Map to MPS\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 2Ô∏è‚É£ Define LoRA PEFT Configuration\n",
    "# ==============================\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # Rank of LoRA\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.05,  # Dropout for stability\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Target attention layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply PEFT to the model\n",
    "model = get_peft_model(base_model, peft_config)\n",
    "model.print_trainable_parameters()  # Verify trainable params\n",
    "\n",
    "# ==============================\n",
    "# 3Ô∏è‚É£ Load Custom Dataset\n",
    "# ==============================\n",
    "# Ensure dataset is formatted as {\"instruction\": \"...\", \"input\": \"...\", \"output\": \"...\"}\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset/dataset.json\")\n",
    "\n",
    "# Format the dataset into instruction-tuning format\n",
    "def format_data(example):\n",
    "    if example[\"input\"]:  # Include input if present\n",
    "        return {\"text\": f\"### Instruction: {example['instruction']}\\n### Input: {example['input']}\\n### Response: {example['output']}\"}\n",
    "    return {\"text\": f\"### Instruction: {example['instruction']}\\n### Response: {example['output']}\"}\n",
    "\n",
    "dataset = dataset.map(format_data)\n",
    "\n",
    "# ==============================\n",
    "# 4Ô∏è‚É£ Define Training Arguments\n",
    "# ==============================\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./fine-tuned-llama\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_strategy=\"no\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    # eval_steps=500,\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    max_steps=-1,  # Adjust based on dataset size\n",
    "    bf16=True,  # Use fp16 instead of bf16 for cuda\n",
    "    optim=\"adamw_torch\",\n",
    "    push_to_hub=False,\n",
    "    max_seq_length=512,\n",
    "    packing = False\n",
    ")\n",
    "\n",
    "# ==============================\n",
    "# 5Ô∏è‚É£ Train the Model with SFTTrainer\n",
    "# ==============================\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset['train'],\n",
    "    processing_class=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ==============================\n",
    "# 6Ô∏è‚É£ Save the Fine-Tuned Model\n",
    "# ==============================\n",
    "trainer.save_model(\"./fine-tuned-llama\")\n",
    "tokenizer.save_pretrained(\"./fine-tuned-llama\")\n",
    "\n",
    "print(\"Fine-tuning complete! Model saved at ./fine-tuned-llama\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef97e6-705d-4ccb-86b8-c2c9c8dbf6f8",
   "metadata": {},
   "source": [
    "## Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e3349c-482a-448f-b30b-79f6e4e8d945",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, './fine-tuned-llama')\n",
    "model = model.to(torch.device(\"mps\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd947f18-6ad1-4156-b479-62a915c07b75",
   "metadata": {},
   "source": [
    "## Test with a sample prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf26a3c8-fcb8-4ffe-8c18-3a6fd7431601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example instruction prompt\n",
    "input_text = \"### Instruction: Which player is going to get most points in next gameweek in FPL?\\n### Response:\"\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move to GPU if available\n",
    "\n",
    "# Generate response\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_new_tokens=200, temperature=0.7, top_p=0.9)\n",
    "\n",
    "# Decode and print response\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"\\nüìù Model Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf3a415-482e-4f60-845a-f117f86e84fd",
   "metadata": {},
   "source": [
    "## Test in a chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83143451-34a7-4336-b2e9-6d024f9f3203",
   "metadata": {},
   "outputs": [],
   "source": [
    "@spaces.GPU\n",
    "def bot_streaming(message, history, max_new_tokens=250):\n",
    "    \n",
    "    txt = message[\"text\"]\n",
    "\n",
    "    messages= [] \n",
    "    images = []\n",
    "    \n",
    "\n",
    "    for i, msg in enumerate(history): \n",
    "        if isinstance(msg[0], tuple):\n",
    "            messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": history[i+1][0]}, {\"type\": \"image\"}]})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": history[i+1][1]}]})\n",
    "            images.append(Image.open(msg[0][0]).convert(\"RGB\"))\n",
    "        elif isinstance(history[i-1], tuple) and isinstance(msg[0], str):\n",
    "            # messages are already handled\n",
    "            pass\n",
    "        elif isinstance(history[i-1][0], str) and isinstance(msg[0], str): # text only turn\n",
    "            messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": msg[0]}]})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": msg[1]}]})\n",
    "\n",
    "    # add current message\n",
    "    if len(message[\"files\"]) == 1:\n",
    "        \n",
    "        if isinstance(message[\"files\"][0], str): # examples\n",
    "            image = Image.open(message[\"files\"][0]).convert(\"RGB\")\n",
    "        else: # regular input\n",
    "            image = Image.open(message[\"files\"][0][\"path\"]).convert(\"RGB\")\n",
    "        images.append(image)\n",
    "        messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": txt}, {\"type\": \"image\"}]})\n",
    "    else:\n",
    "        messages.append({\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": txt}]})\n",
    "\n",
    "    texts = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "    if images == []:\n",
    "        inputs = processor(text=texts, return_tensors=\"pt\").to(\"mps\")\n",
    "    else:\n",
    "        inputs = processor(text=texts, images=images, return_tensors=\"pt\").to(\"mps\")\n",
    "    streamer = transformers.TextIteratorStreamer(processor, skip_special_tokens=True, skip_prompt=True)\n",
    "\n",
    "    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=max_new_tokens)\n",
    "    generated_text = \"\"\n",
    "    \n",
    "    thread = Thread(target=MODEL.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "    buffer = \"\"\n",
    "    \n",
    "    for new_text in streamer:\n",
    "        buffer += new_text\n",
    "        generated_text_without_prompt = buffer\n",
    "        time.sleep(0.01)\n",
    "        yield buffer\n",
    "\n",
    "\n",
    "MODEL = model\n",
    "demo = gr.ChatInterface(fn=bot_streaming, title=\"Multimodal Llama\",\n",
    "          textbox=gr.MultimodalTextbox(), \n",
    "          additional_inputs = [gr.Slider(\n",
    "                  minimum=10,\n",
    "                  maximum=150,\n",
    "                  value=250,\n",
    "                  step=10,\n",
    "                  label=\"Maximum number of new tokens to generate\",\n",
    "              )\n",
    "            ],\n",
    "          cache_examples=False,\n",
    "          description=\"Test LLAMA\",\n",
    "          stop_btn=\"Stop Generation\", \n",
    "          fill_height=False,\n",
    "        multimodal=True)\n",
    "        \n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7855464-4114-42ad-b2d8-ce0018e1930d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
